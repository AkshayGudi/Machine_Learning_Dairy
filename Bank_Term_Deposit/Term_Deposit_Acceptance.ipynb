{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Term Deposit Tensor Flow: Checking if person accepts Term Deposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(filepath_or_buffer=\"bank_info.csv\", sep=\";\")\n",
    "\n",
    "X = dataset.iloc[:,[0,1,2,3,4,5,6,7,11,12,13,14]]\n",
    "y = dataset.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categroical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode categroical data\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[1])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#Avoid dummy variable trap\n",
    "X = X[:,1:]\n",
    "\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[12])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#Avoid dummy variable trap\n",
    "X = X[:,1:]\n",
    "\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[14])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#Avoid dummy variable trap\n",
    "X = X[:,1:]\n",
    "\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[17])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#Avoid dummy variable trap\n",
    "X = X[:,1:]\n",
    "\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[19])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#Avoid dummy variable trap\n",
    "X = X[:,1:]\n",
    "\n",
    "\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[20])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X), dtype=np.float)\n",
    "\n",
    "#Avoid dummy variable trap\n",
    "X = X[:,1:]\n",
    "\n",
    "\n",
    "ct = ColumnTransformer([('encoder',OneHotEncoder(),[0])], remainder=\"passthrough\")\n",
    "y = np.array(ct.fit_transform(y),dtype=np.int)\n",
    "y = y[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X  = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data into Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Fitting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the ANN: Defining sequence of layers\n",
    "classifier = Sequential()\n",
    "\n",
    "#Adding the input and first hidden layer\n",
    "classifier.add(Dense(units = 14, kernel_initializer='uniform', activation='relu', input_dim=25))\n",
    "\n",
    "#Adding second hidden layer\n",
    "classifier.add(Dense(units = 14, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "#Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "#Compiling ANN (SGD)\n",
    "classifier.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#fitting the ANN to training set\n",
    "classifier.fit(X_train,y_train,batch_size=10,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion metrics\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "#Accuracy\n",
    "acc = (cm[0][0] + cm[1][1])/ np.sum(cm)\n",
    "print(\"Accurancy is {0}\".format(acc*100))\n",
    "\n",
    "#Recall\n",
    "recall = cm[1][1]/(cm[1][1] + cm[1][0])\n",
    "print(\"Recall is {0}\".format(recall*100))\n",
    "\n",
    "#Precision\n",
    "prec = cm[1][1]/(cm[1][1] + cm[0][1])\n",
    "print(\"Precision is {0}\".format(prec*100))\n",
    "\n",
    "#F1 Score\n",
    "f1_score = 2 * prec * recall/(prec+recall)\n",
    "print(\"F1 Score {0}\".format(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
